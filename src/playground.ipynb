{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML_ECOL_playground.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOgYTehZsKCUgi25iBeG67X"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yHXaa6_TcNvV","executionInfo":{"status":"ok","timestamp":1639285041884,"user_tz":-60,"elapsed":3291,"user":{"displayName":"Gojko Cutura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07120011660917555031"}},"outputId":"38713c48-1dcf-4125-feae-231f76c6d5d6"},"source":["!pip install rawpy"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: rawpy in /usr/local/lib/python3.7/dist-packages (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rawpy) (1.19.5)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LVmVAOWkeHHv","executionInfo":{"status":"ok","timestamp":1639290660075,"user_tz":-60,"elapsed":4142,"user":{"displayName":"Gojko Cutura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07120011660917555031"}},"outputId":"8c7c4a5e-a695-4bad-9a08-57280ca8b81c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","PROJECT_ROOT = '/content/drive/MyDrive/ML_ECOL_project'\n","os.chdir(PROJECT_ROOT)\n","\n","DATA_PATH = os.path.join(PROJECT_ROOT, 'data')\n","DATASET_PATH = os.path.join(PROJECT_ROOT, 'dataset')\n","CHECKPOINTS_PATH = os.path.join(PROJECT_ROOT, 'checkpoints')\n","os.makedirs(CHECKPOINTS_PATH, exist_ok=True)\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data.dataset import Dataset\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","\n","import numpy as np\n","import matplotlib.pyplot as plt \n","import cv2\n","import rawpy \n","import imageio \n","from PIL import Image\n","from tqdm import tqdm\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"CH32NzVbtl0b"},"source":["# === NEF - JPG ==="]},{"cell_type":"code","metadata":{"id":"RKq0AvYgxq5s"},"source":["class NEFConverter():\n","\n","    def find_nef_images(self, root_dir):\n","        self.root_dir = root_dir\n","\n","        subdir_paths = []\n","        for subdir in os.listdir(self.root_dir):\n","            if os.path.isdir(os.path.join(self.root_dir, subdir)):\n","                subdir_paths.append(os.path.join(self.root_dir, subdir))\n","\n","        self.img_paths = []\n","        for subdir_path in subdir_paths:\n","            for img_name in os.listdir(subdir_path):\n","                _, extension = os.path.splitext(img_name)\n","                if extension == '.nef':\n","                    self.img_paths.append(os.path.join(subdir_path, img_name))\n","\n","\n","    def convert(self):\n","        for i, img_path in enumerate(self.img_paths):\n","\n","            print(f'Image {i}/{len(self.img_paths)}')\n","            path_wo_extension, extension = os.path.splitext(img_path)\n","            save_path_jpg = f'{path_wo_extension}.jpg'\n","\n","            if os.path.isfile(save_path_jpg):\n","                print(f'Already exists: {save_path_jpg}')\n","                print('Skipping\\n')\n","                continue\n","\n","            with rawpy.imread(img_path) as raw:\n","                rgb = raw.postprocess()\n","\n","                print(f'Saving to {save_path_jpg}')\n","                imageio.imsave(save_path_jpg, rgb)\n","                print('Done\\n')\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RrjVyBO5z4IN"},"source":["converter = NEFConverter()\n","converter.find_nef_images(DATA_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EVNzcyVuz38C","executionInfo":{"status":"ok","timestamp":1639273427198,"user_tz":-60,"elapsed":384,"user":{"displayName":"Gojko Cutura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07120011660917555031"}}},"source":["from time import time \n","\n","start = time()\n","converter.convert()\n","end = time()\n","\n","print(end - start)\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"A9ext8TF1fHA"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-C8HiNHa1fD3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0nqyiOxS7iXz"},"source":["# === MODEL ==="]},{"cell_type":"code","metadata":{"id":"GgrZa2vmVyJ1","executionInfo":{"status":"ok","timestamp":1639290668568,"user_tz":-60,"elapsed":1415,"user":{"displayName":"Gojko Cutura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07120011660917555031"}}},"source":["class AEContractingBlock(nn.Module):\n","    \n","    def __init__(\n","        self, in_channels, out_channels=None, \n","        use_bn = True, use_dropout = False\n","        ):\n","        super(AEContractingBlock, self).__init__()\n","        \n","        if out_channels is None:\n","            out_channels = 2 * in_channels\n","        self.use_bn = use_bn\n","        self.use_dropout = use_dropout\n","\n","        self.conv = nn.Conv2d(\n","            in_channels, out_channels, \n","            kernel_size = 4, stride = 2, padding = 1\n","            )\n","        \n","        if self.use_bn:\n","            self.bn = nn.BatchNorm2d(out_channels)\n","        if self.use_dropout:\n","            self.dropout = nn.Dropout()\n","        self.activation = nn.LeakyReLU(0.2)\n","\n","\n","    def forward(self, x):\n","\n","        out = self.conv(x)\n","        if self.use_bn:\n","            out = self.bn(out)\n","        if self.use_dropout:\n","            out = self.dropout(out)\n","        out = self.activation(out)\n","\n","        return out\n","\n","\n","\n","class AEExpandingBlock(nn.Module):\n","\n","    def __init__(\n","        self, in_channels, out_channels=None, \n","        use_bn = True, use_dropout = False\n","        ):\n","        super(AEExpandingBlock, self).__init__()\n","\n","        if out_channels is None:\n","            out_channels = in_channels // 2\n","        self.use_bn = use_bn\n","        self.use_dropout = use_dropout\n","\n","        self.conv = nn.ConvTranspose2d(\n","            in_channels, out_channels, \n","            kernel_size = 4, stride = 2, padding = 1\n","            )\n","        \n","        if self.use_bn:\n","            self.bn = nn.BatchNorm2d(out_channels)\n","        if self.use_dropout:\n","            self.dropout = nn.Dropout()\n","        self.activation = nn.ReLU() \n"," \n","\n","    def forward(self, x):\n","        \n","        out = self.conv(x)\n","        if self.use_bn:\n","            out = self.bn(out)\n","        if self.use_dropout:\n","            out = self.dropout(out)\n","        out = self.activation(out)\n","\n","        return out\n","\n","\n","\n","class Encoder(nn.Module):\n","\n","    def __init__(self, in_channels, hidden_channels, depth):\n","        super(Encoder, self).__init__()\n","\n","        self.depth = depth\n","\n","        # Input -> hidden mapping\n","        self.set_hidden_channels = nn.Conv2d(\n","            in_channels, hidden_channels, kernel_size=1\n","            )\n","\n","        # Contracting layers\n","        self.contracting_layers = nn.ModuleList()\n","        start_scale = int(hidden_channels / 2)\n","        contracting_scales = [2*i*start_scale for i in range(1, self.depth + 2)]\n","        for i in range(self.depth):\n","            curr_in_channels = contracting_scales[i]\n","            curr_out_channels = contracting_scales[i + 1]\n","\n","            self.contracting_layers.append(\n","                AEContractingBlock(\n","                    curr_in_channels, curr_out_channels, use_dropout=True\n","                    )\n","                )\n","            \n","\n","    def forward(self, x):\n","        out = self.set_hidden_channels(x)\n","\n","        for i in range(self.depth):\n","            out = self.contracting_layers[i](out)\n","\n","        return out\n","\n","\n","\n","class Decoder(nn.Module):\n","\n","    def __init__(self, hidden_channels, out_channels, depth):\n","        super(Decoder, self).__init__()\n","\n","        self.depth = depth\n","\n","        # Expanding layers\n","        self.expanding_layers = nn.ModuleList()\n","        start_scale = int(hidden_channels / 2)\n","        expanding_scales = [2*i*start_scale for i in range(1, self.depth + 2)]\n","        expanding_scales = expanding_scales[ : : -1]\n","        for i in range(self.depth):\n","            curr_in_channels = expanding_scales[i]\n","            curr_out_channels = expanding_scales[i + 1]\n","            \n","            self.expanding_layers.append(\n","                AEExpandingBlock(\n","                    curr_in_channels, curr_out_channels, use_dropout=True\n","                    )\n","                )\n","\n","        # Hidden -> output mapping\n","        self.set_output_channels = nn.Conv2d(\n","            hidden_channels, out_channels, kernel_size=1\n","            )\n","    \n","\n","    def forward(self, x):\n","        out = x\n","        for i in range(self.depth):\n","            out = self.expanding_layers[i](out)\n","\n","        out = self.set_output_channels(out)\n","\n","        return out\n","\n","\n","\n","class AutoencoderCNN(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels, hidden_channels=64, depth=4):\n","        super(AutoencoderCNN, self).__init__()\n","\n","        self.depth = depth\n","        self.checkpoint_dir = CHECKPOINTS_PATH\n","\n","        # Encoder\n","        self.encoder = Encoder(in_channels, hidden_channels, depth)\n","        # Decoder \n","        self.decoder = Decoder(hidden_channels, out_channels, depth)\n","\n"," \n","    def forward(self, x):\n","        encoding = self.encoder(x)\n","        out = self.decoder(encoding)\n","        \n","        return {'out': out, 'encoding': encoding}\n","\n","    \n","    def train_epoch(self, loader, optimizer, epoch_idx, device, max_iters=None):\n","        self.train()\n","        batch_size = loader.batch_size\n","        running_loss = 0.0\n","\n","        train_len = len(loader) if max_iters is None else max_iters\n","\n","        progress_bar = tqdm(\n","            loader, total=train_len, desc=f'Epoch {epoch_idx}'\n","            )\n","\n","        for batch_idx, data in enumerate(progress_bar):\n","            if max_iters is not None:\n","                if batch_idx == max_iters:\n","                    break\n","\n","            data = data.to(device)\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            output = self(data)['out']\n","            loss = nn.functional.mse_loss(output, data)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # log statistics\n","            running_loss += loss.item()\n","            avg_loss = running_loss / (batch_idx + 1)\n","            progress_bar.set_postfix(loss=avg_loss, stage=\"train\")\n","        \n","        if max_iters is None:\n","            return running_loss / len(loader)\n","        else:\n","            return running_loss / max_iters \n","\n","\n","    def evaluate(self, loader, device, epoch_idx=None, should_print=True):        \n","        self.eval()\n","        test_loss, correct = 0, 0\n","\n","        if epoch_idx is None:\n","          description = 'Evaluating'\n","        else:\n","          description = f'Validation epoch {epoch_idx}'\n","\n","        progress_bar = tqdm(loader, total=len(loader), desc=description)\n","        \n","        with torch.no_grad():\n","            for batch_idx, data in enumerate(progress_bar):\n","                data = data.to(device)\n","\n","                output = self(data)['out']\n","\n","                loss = nn.functional.cross_entropy(output, data)\n","                test_loss += loss.item()\n","                avg_loss = test_loss / (batch_idx + 1)\n","\n","                progress_bar.set_postfix(loss=avg_loss, stage=\"validation\")\n","\n","        test_loss /= len(loader.dataset)\n","\n","        if should_print:\n","          print(f'\\nAvg. loss: {test_loss:.4f}')\n","\n","        return test_loss, accuracy\n","\n","\n","    def save_checkpoint(self, epoch=None, checkpoint_name=None):\n","        if checkpoint_name is None:\n","            checkpoint_name = 'latest_checkpoint.pt'\n","        save_path = os.path.join(self.checkpoint_dir, checkpoint_name)\n","        torch.save({'model': self, 'epoch': str(epoch)}, save_path)\n","\n","\n","    def plot_progress(\n","        self, \n","        loader, \n","        num_batches, \n","        save_dir, \n","        device, \n","        show_plot=False, \n","        normalized=True\n","        ):\n","\n","        self.eval()\n","\n","        for i, data in enumerate(loader):\n","            if i == num_batches:\n","                break \n","            data = data.to(device)\n","            pred = self(data)['out']\n","\n","            in_out_batch = torch.concat([data, pred], dim=0)\n","            grid_img = torchvision.utils.make_grid(\n","                in_out_batch, nrow=loader.batch_size\n","                )\n","            \n","            os.makedirs(save_dir, exist_ok=True)\n","            save_path = os.path.join(save_dir, f'batch_{i}.png')\n","\n","            figsize = (4 * loader.batch_size, 10)\n","            show_image(\n","                grid_img.cpu(), \n","                normalized=normalized, \n","                figsize=figsize, \n","                save_path=save_path, \n","                show_plot=show_plot\n","                )\n","\n","\n","    def fit(self, loaders, optimizer, epochs, device, max_iters=None):\n","        train_loader, val_loader = loaders\n","\n","        train_losses, val_losses = [], []\n","        for epoch in range(epochs):\n","            # Training epoch\n","            running_loss = self.train_epoch(\n","                loader=train_loader, \n","                optimizer=optimizer, \n","                epoch_idx=epoch,\n","                device=device,\n","                max_iters=max_iters\n","                )\n","            train_losses.append(running_loss)\n","\n","            # Validation\n","            if val_loader is not None:\n","                val_loss, _ = self.evaluate(\n","                    val_loader, device, should_print=False, epoch_idx=epoch\n","                    )\n","                val_losses.append(val_loss)\n","\n","            # Save checkpoint\n","            self.save_checkpoint(epoch=epoch)\n","\n","            # Plot current output images\n","            progress_plot_dir = os.path.join(\n","                CHECKPOINTS_PATH, 'plots', f'progress_epoch_{epoch}'\n","                )\n","            self.plot_progress(train_loader, 5, progress_plot_dir, device)\n","\n","        self.latest_train_losses = train_losses\n","        self.latest_val_losses = val_losses\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yLIeolOh7oA1"},"source":["# === DATASET ==="]},{"cell_type":"code","metadata":{"id":"I9D2wvSWI7Hk","executionInfo":{"status":"ok","timestamp":1639290669167,"user_tz":-60,"elapsed":2,"user":{"displayName":"Gojko Cutura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07120011660917555031"}}},"source":["class UnNormalize(object):\n","  def __init__(self, mean, std):\n","    self.mean = mean\n","    self.std = std\n","\n","  def __call__(self, tensor):\n","    \"\"\"\n","    Args:\n","      tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n","    Returns:\n","      Tensor: Normalized image.\n","    \"\"\"\n","    for t, m, s in zip(tensor, self.mean, self.std):\n","      t.mul_(s).add_(m)\n","      # The normalize code -> t.sub_(m).div_(s)\n","    return tensor\n","\n","\n","def show_image(img, figsize=(9, 6), normalized=False, save_path=None, show_plot=True):\n","    MEANS = (0.485, 0.456, 0.406)\n","    STDS = (0.229, 0.224, 0.225)\n","    unorm = UnNormalize(MEANS, STDS)\n","\n","    img_to_show = img if not normalized else unorm(img)\n","\n","    plt.figure(figsize=figsize)    \n","    plt.imshow(img_to_show.permute(1, 2, 0))\n","    if save_path is not None:\n","        plt.savefig(save_path)\n","    if show_plot:\n","        plt.show()\n","    plt.close()"],"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class ECOLDataset(Dataset):\n","\n","    def __init__(self, root_dir, transform=None, img_size=(512, 512)):\n","        super(ECOLDataset, self).__init__()\n","\n","        self.root_dir = os.path.abspath(root_dir)\n","        self.MEANS = (0.485, 0.456, 0.406)\n","        self.STDS = (0.229, 0.224, 0.225)\n","        self.img_size = img_size\n","\n","        # Iterate through the dataset iterator and count the elements\n","        dataset_iterator = os.scandir(root_dir)\n","        i = -1\n","        for i, _ in enumerate(dataset_iterator):\n","            pass\n","\n","        self.len = i + 1\n","\n","        # Torchvision transformation to be applied to each image\n","        if transform is not None:\n","            self.transform = transform \n","        else:\n","            self.transform = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Resize(self.img_size),\n","                transforms.Normalize(mean=self.MEANS, std=self.STDS),\n","                ])\n","            \n","\n","    def __len__(self):\n","        # Has to exist in order to inherit the parent class properly!\n","        return self.len\n","\n","\n","    def __getitem__(self, idx):\n","\n","        dataset_iterator = os.scandir(self.root_dir)\n","        img_path = None\n","        for i, curr_img_name in enumerate(dataset_iterator):\n","            if i == idx:\n","                img_path = os.path.join(self.root_dir, curr_img_name.name)\n","                break\n","\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        transformed_image = self.transform(image)\n","\n","        return transformed_image\n","\n","\n","\n","def load_dataset_ECOL(\n","    root_dir, \n","    img_size, \n","    batch_size, \n","    num_workers=0, \n","    shuffle=True,\n","    transform=None,\n","    ):\n","\n","    dataset = ECOLDataset(root_dir, transform, img_size) \n","\n","    # Create DataLoader object for the dataset  \n","    loader = DataLoader(\n","        dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers\n","        ) \n","\n","    return loader"],"metadata":{"id":"gH-IFFu8Uo-M","executionInfo":{"status":"ok","timestamp":1639290671026,"user_tz":-60,"elapsed":192,"user":{"displayName":"Gojko Cutura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07120011660917555031"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"T6HwpqCuVyHo","executionInfo":{"status":"ok","timestamp":1639290702686,"user_tz":-60,"elapsed":191,"user":{"displayName":"Gojko Cutura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07120011660917555031"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6dba1c39-c57e-42b0-cc5e-c6507227f925"},"source":["IN_CHANNELS = 3\n","OUT_CHANNELS = 3\n","HIDDEN_CHANNELS = 2\n","DEPTH = 4\n","EPOCHS = 10\n","LEARNING_RATE = 0.001\n","MAX_ITERS = 1000\n","\n","AE_PARAMS = (IN_CHANNELS, OUT_CHANNELS, HIDDEN_CHANNELS, DEPTH)\n","\n","LOADER_PARAMS = {\n","    'root_dir': DATASET_PATH, 'img_size': (256, 256), 'batch_size': 4\n","    }\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(DEVICE)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","metadata":{"id":"ZFzTOY2ohHx2","executionInfo":{"status":"ok","timestamp":1639290704335,"user_tz":-60,"elapsed":420,"user":{"displayName":"Gojko Cutura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07120011660917555031"}}},"source":["# Loading the Data Loader\n","dataloader = load_dataset_ECOL(**LOADER_PARAMS)\n","\n","# Loading the AE model \n","model = AutoencoderCNN(*AE_PARAMS)\n","model = model.to(DEVICE)\n","\n","# Loading the Adam optimizer\n","optimizer = torch.optim.Adam(model.parameters(), LEARNING_RATE)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HHyeTt2HRNUm","outputId":"a874ee0d-847f-4f92-fb69-2b80e9e9b054"},"source":["model.fit([dataloader, None], optimizer, EPOCHS, DEVICE, MAX_ITERS)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 0: 100%|██████████| 1000/1000 [35:14<00:00,  2.11s/it, loss=0.219, stage=train]\n","Epoch 1: 100%|██████████| 1000/1000 [31:58<00:00,  1.92s/it, loss=0.171, stage=train]\n","Epoch 2: 100%|██████████| 1000/1000 [28:18<00:00,  1.70s/it, loss=0.164, stage=train]\n","Epoch 3:  76%|███████▌  | 760/1000 [20:17<06:51,  1.71s/it, loss=0.155, stage=train]"]}]},{"cell_type":"markdown","source":["# === UNUSED STUFF === "],"metadata":{"id":"q_ahAvolUdGc"}},{"cell_type":"code","source":["loaded = torch.load(os.path.join(CHECKPOINTS_PATH, 'latest_checkpoint.pt'))\n","loaded_model = loaded['model'].to(DEVICE)\n"],"metadata":{"id":"CbulRy_mKU_L","executionInfo":{"status":"ok","timestamp":1639289912933,"user_tz":-60,"elapsed":277,"user":{"displayName":"Gojko Cutura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07120011660917555031"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","metadata":{"id":"q6Pj1E105ooY","executionInfo":{"status":"ok","timestamp":1639290543218,"user_tz":-60,"elapsed":2537,"user":{"displayName":"Gojko Cutura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07120011660917555031"}}},"source":["pred = loaded_model(next(iter(dataloader)).to(DEVICE))"],"execution_count":110,"outputs":[]},{"cell_type":"code","source":["pred['encoding'].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sIE21tM3UgG5","executionInfo":{"status":"ok","timestamp":1639290544356,"user_tz":-60,"elapsed":218,"user":{"displayName":"Gojko Cutura","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07120011660917555031"}},"outputId":"db7c7217-97aa-4c05-ba6e-5c2e82c1e9ed"},"execution_count":111,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 10, 16, 16])"]},"metadata":{},"execution_count":111}]},{"cell_type":"code","source":[""],"metadata":{"id":"4ZS6CcpTUgEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rki_W4csVx_0"},"source":["class ECOLDatasetRaw(Dataset):\n","\n","    def __init__(\n","        self, \n","        root_dir, \n","        transform=None, \n","        extensions=['.jpg', '.nef'],\n","        img_size=(512, 512)\n","        ):\n","        super(ECOLDatasetRaw, self).__init__()\n","\n","        self.root_dir = os.path.abspath(root_dir)\n","        self.extensions = extensions\n","        self.MEANS = (0.485, 0.456, 0.406)\n","        self.STDS = (0.229, 0.224, 0.225)\n","        self.img_size = img_size\n","\n","        subdir_paths = []\n","        for subdir in os.listdir(self.root_dir):\n","            if os.path.isdir(os.path.join(self.root_dir, subdir)):\n","                subdir_paths.append(os.path.join(self.root_dir, subdir))\n","\n","        self.img_paths = []\n","        for subdir_path in subdir_paths:\n","            for img_name in os.listdir(subdir_path):\n","                _, extension = os.path.splitext(img_name)\n","                if extension in self.extensions:\n","                    self.img_paths.append(os.path.join(subdir_path, img_name))\n","\n","        self.len = len(self.img_paths)\n","\n","        if transform is not None:\n","            self.transform = transform \n","        else:\n","            self.transform = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Resize(self.img_size),\n","                transforms.Normalize(mean=self.MEANS, std=self.STDS),\n","                ])\n","            \n","\n","    def __len__(self):\n","        # Has to exist in order to inherit the parent class properly!\n","        return self.len\n","\n","\n","    def __getitem__(self, idx):\n","        \n","        img_path = self.img_paths[idx]\n","\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        transformed_image = self.transform(image)\n","\n","        return transformed_image\n","\n","\n","\n","def load_dataset_ECOL_raw(\n","    root_dir, \n","    img_size, \n","    batch_size, \n","    num_workers=0, \n","    shuffle=True,\n","    transform=None,\n","    extensions=['.jpg']\n","    ):\n","\n","    dataset = ECOLDatasetRaw(\n","        root_dir=root_dir, \n","        img_size=img_size, \n","        transform=transform, \n","        extensions=extensions\n","        ) \n","\n","    # Create DataLoader object for the dataset  \n","    train_loader = DataLoader(\n","        dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers\n","        ) \n","\n","    return train_loader"],"execution_count":null,"outputs":[]}]}